import openai
import pandas as pd
import json
import yaml
from functools import lru_cache
from openai import OpenAI


@lru_cache(maxsize=1)
def load_configs():
    """
    Loads the configuration from 'configs.yaml' into a global dictionary (YAML_DICT).
    Uses LRU cache to ensure the file is only loaded once per process.
    """
    global YAML_DICT
    print("Loading configs.yaml file...")
    # Open and read the YAML configuration file
    with open("configs.yaml", "r") as file:
        YAML_DICT = yaml.safe_load(file)


def get_configs(name, type):
    """
    Retrieves a specific configuration value from the loaded YAML configuration dictionary.

    Args:
        name (str): The top-level key in the YAML configuration (e.g., 'conversation').
        type (str): The sub-key under the top-level key (e.g., 'model', 'tools').

    Returns:
        The value corresponding to the specified name and type from the YAML configuration.

    Raises:
        KeyError: If the specified name or type does not exist in the configuration.
    """
    global YAML_DICT
    load_configs()  # Ensure the configuration is loaded (cached after first load)

    try:
        if name not in YAML_DICT:
            raise KeyError(f"Top-level key '{name}' not found in configs.yaml.")
        if type not in YAML_DICT[name]:
            raise KeyError(f"Sub-key '{type}' not found under '{name}' in configs.yaml.")
        return YAML_DICT[name][type]
    except KeyError:
        # Raise a descriptive error if the configuration is missing
        raise KeyError(f"Configuration '{name}' with type '{type}' not found in configs.yaml.")


def get_chat_completions(conversation_bot):
    """
    Calls the OpenAI API to generate chat completions based on the provided conversation history.

    Args:
        conversation_bot (list): A list of dictionaries representing the conversation history. 
                                 Each dictionary contains keys like 'type', 'call_id', 'name', 
                                 'arguments', or 'output'.

    Returns:
        str: The output text generated by the OpenAI API based on the conversation history.
    """
    print("OpenAI API call: ", conversation_bot[-1])

    # Initialize the OpenAI client
    client = OpenAI()
    # Call the OpenAI API to get chat completions based on the input conversation
    response = client.responses.create(
        model=get_configs('ShopAssist', 'model'),
        input=conversation_bot,
        tools=get_configs('ShopAssist', 'tools')
    )

    # Loop to handle function calls returned by the model
    while response.output[0].type == "function_call":
        name = response.output[0].name
        args = json.loads(response.output[0].arguments)
        call_id = response.output[0].call_id

        print('Function_call: ', name, 'arguments:', args)
        # If the function call is 'recommend_laptops', execute it and append the output to the conversation
        if name == 'recommend_laptops':
            conversation_bot.append({'type': 'function_call',
                                     'call_id': call_id,
                                     'name': name,
                                     'arguments': str(args)
            })

            # Function call to recommend laptops based on user requirements
            output = recommend_laptops(**args)
            print('Function call output: ', output)

            # Append the function call output to the conversation
            conversation_bot.append({'type': 'function_call_output',
                                     'call_id': call_id,
                                     'output': str(output)
            })

            # Call the API again with the updated conversation
            response = client.responses.create(
                model=get_configs('ShopAssist', 'model'),
                input=conversation_bot,
                tools=get_configs('ShopAssist', 'tools')
            )
    
    print("OpenAI API output_text: ", response.output_text)

    # Return the final output text from the API
    return response.output_text


def recommend_laptops(**args):
    """
    Recommends laptops based on user-specified requirements.

    Args:
        **args: Arbitrary keyword arguments representing user requirements.
                Expected keys include 'budget' and feature preferences
                (e.g., 'GPU intensity', 'Display quality', etc.).

    Returns:
        str: JSON string of the top 3 recommended laptops that meet or exceed
             the user's requirements, sorted by score.
    """
    # Load the laptop dataset
    laptop_df = pd.read_csv('updated_laptop.csv')

    # Extract the user's budget from the arguments
    budget = args.get('budget')

    # Filter laptops within the user's budget
    filtered_laptops = laptop_df.copy()
    filtered_laptops['Price'] = filtered_laptops['Price'].str.replace(',', '').astype(int)
    filtered_laptops = filtered_laptops[filtered_laptops['Price'] <= budget].copy()

    # Map string values 'low', 'medium', 'high' to numerical scores
    mappings = {'low': 0, 'medium': 1, 'high': 2}

    # Initialize a new column 'Score' to 0
    filtered_laptops['Score'] = 0

    # Iterate over each laptop to calculate scores based on user requirements
    for index, row in filtered_laptops.iterrows():
        # Parse the laptop's feature string into a dictionary
        laptop_config_str = row['laptop_feature']
        laptop_config_str = laptop_config_str.replace("'", '"')  # Ensure valid JSON format
        laptop_config = json.loads(laptop_config_str)
        score = 0

        # Compare each user requirement with the laptop's features
        for user_req_key, user_req_value in args.items():
            if user_req_key == 'budget':
                continue  # Skip budget comparison

            # Increment score if laptop feature meets or exceeds user requirement
            if mappings.get(laptop_config.get(user_req_key, None), -1) >= mappings.get(user_req_value, -1):
                score += 1

        # Update the score for the current laptop
        filtered_laptops.loc[index, 'Score'] = score

    # Filter laptops with a score greater than 2, sort by score, and select top 3
    top_laptops = filtered_laptops.drop('laptop_feature', axis=1)
    top_laptops = top_laptops[top_laptops['Score'] > 2]
    top_laptops = top_laptops.sort_values('Score', ascending=False).head(3)

    # Convert the top laptops DataFrame to JSON format
    top_laptops_json = top_laptops.to_json(orient='records')

    # Return the JSON string of recommended laptops
    return top_laptops_json


def moderation_check(user_input):
    """
    Checks the user's input for content policy violations using the OpenAI Moderation API.

    Args:
        user_input (str): The input string to be checked.

    Returns:
        str: "Flagged" if the input is flagged by moderation, otherwise "Not Flagged".
    """
    # Call the OpenAI Moderation API to analyze the user's input.
    response = openai.moderations.create(input=user_input)

    # Check if the moderation system flagged the input.
    if response.results[0].flagged == True:
        # If flagged, print and return "Flagged"
        print('Moderation check: Flagged')
        return "Flagged"
    else:
        # If not flagged, print and return "Not Flagged"
        print('Moderation check: Not Flagged')
        return "Not Flagged"


def product_map_layer(laptop_description):
    '''
    Note: This function needs to be manually run once to create the product_map_layer.csv file.
    Classifies key laptop features from a given description into categorical levels.
    This function analyzes a laptop's description to extract and classify its primary features:
    GPU intensity, Display quality, Portability, Multitasking, and Processing speed.
    Each feature is classified as 'low', 'medium', or 'high' based on predefined rules.
    Args:
        laptop_description (str): A textual description of the laptop, including specifications such as processor, RAM, display, GPU, and weight.
    Returns:
        dict: A dictionary mapping each feature ('GPU intensity', 'Display quality', 'Portability', 'Multitasking', 'Processing speed')
              to its classified level ('low', 'medium', or 'high').
    '''
    
    delimiter = "#####"

    lap_spec = {
        "GPU intensity":"(Type of the Graphics Processor)",
        "Display quality":"(Display Type, Screen Resolution, Display Size)",
        "Portability":"(Laptop Weight)",
        "Multitasking":"(RAM Size)",
        "Processing speed":"(CPU Type, Core, Clock Speed)"
    }

    values = {'low','medium','high'}

    prompt=f"""
    You are a Laptop Specifications Classifier whose job is to extract the key features of laptops and classify them as per their requirements.
    To analyze each laptop, perform the following steps:
    Step 1: Extract the laptop's primary features from the description {laptop_description}
    Step 2: Store the extracted features in {lap_spec} \
    Step 3: Classify each of the items in {lap_spec} into {values} based on the following rules: \
    {delimiter}
    GPU Intensity:
    - low: <<< if GPU is entry-level such as an integrated graphics processor or entry-level dedicated graphics like Intel UHD >>> , \n
    - medium: <<< if mid-range dedicated graphics like M1, AMD Radeon, Intel Iris >>> , \n
    - high: <<< high-end dedicated graphics like Nvidia RTX >>> , \n

    Display Quality:
    - low: <<< if resolution is below Full HD (e.g., 1366x768). >>> , \n
    - medium: <<< if Full HD resolution (1920x1080) or higher. >>> , \n
    - high: <<< if High-resolution display (e.g., 4K, Retina) with excellent color accuracy and features like HDR support. >>> \n

    Portability:
    - high: <<< if laptop weight is less than 1.51 kg >>> , \n
    - medium: <<< if laptop weight is between 1.51 kg and 2.51 kg >>> , \n
    - low: <<< if laptop weight is greater than 2.51 kg >>> \n

    Multitasking:
    - low: <<< If RAM size is 8 GB, 12 GB >>> , \n
    - medium: <<< if RAM size is 16 GB >>> , \n
    - high: <<< if RAM size is 32 GB, 64 GB >>> \n

    Processing Speed:
    - low: <<< if entry-level processors like Intel Core i3, AMD Ryzen 3 >>> , \n
    - medium: <<< if Mid-range processors like Intel Core i5, AMD Ryzen 5 >>> , \n
    - high: <<< if High-performance processors like Intel Core i7, AMD Ryzen 7 or higher >>> \n
    {delimiter}

    {delimiter}
    Here is input output pair for few-shot learning:
    input 1: "The Dell Inspiron is a versatile laptop that combines powerful performance and affordability. It features an Intel Core i5 processor clocked at 2.4 GHz, ensuring smooth multitasking and efficient computing. With 8GB of RAM and an SSD, it offers quick data access and ample storage capacity. The laptop sports a vibrant 15.6" LCD display with a resolution of 1920x1080, delivering crisp visuals and immersive viewing experience. Weighing just 2.5 kg, it is highly portable, making it ideal for on-the-go usage. Additionally, it boasts an Intel UHD GPU for decent graphical performance and a backlit keyboard for enhanced typing convenience. With a one-year warranty and a battery life of up to 6 hours, the Dell Inspiron is a reliable companion for work or entertainment. All these features are packed at an affordable price of 35,000, making it an excellent choice for budget-conscious users."
    output 1: {{'GPU intensity': 'medium','Display quality':'medium','Portability':'medium','Multitasking':'high','Processing speed':'medium'}}

    {delimiter}
    ### Strictly don't keep any other text in the values of the JSON dictionary other than low or medium or high ###
    """
    input = f"""Follow the above instructions step-by-step and output the dictionary in JSON format {lap_spec} for the following laptop {laptop_description}."""
    #see that we are using the Completion endpoint and not the Chatcompletion endpoint
    messages=[{"role": "system", "content":prompt },{"role": "user","content":input}]

    response = get_chat_completions(messages, json_format = True)

    #   response = openai.chat.completions.create(
    #     model="gpt-3.5-turbo-0125",
    #     messages=[{"role": "system", "content":prompt },{"role": "user","content":input}],
    #     response_format={ "type": "json_object" }
    #     # max_tokens = 2000,
    #     )

    # response = json.loads(response)
    return response